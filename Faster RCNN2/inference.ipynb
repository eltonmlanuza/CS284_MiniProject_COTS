{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T00:10:18.289520Z",
     "iopub.status.busy": "2022-01-19T00:10:18.288858Z",
     "iopub.status.idle": "2022-01-19T00:10:22.359096Z",
     "shell.execute_reply": "2022-01-19T00:10:22.358310Z",
     "shell.execute_reply.started": "2022-01-19T00:10:18.289428Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "\n",
    "# Very few imports. This is a pure torch solution!\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "#WEIGHTS_FILE = \"../working/fasterrcnn_resnet50_fpn-e7.bin\"\n",
    "WEIGHTS_FILE = \"../input/rcnnmodel3/fasterrcnn_resnet50_fpn-e99.bin\"\n",
    "from PIL import Image\n",
    "import PIL\n",
    "\n",
    "# Very few imports. This is a pure torch solution!\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T00:10:22.362787Z",
     "iopub.status.busy": "2022-01-19T00:10:22.362569Z",
     "iopub.status.idle": "2022-01-19T00:10:29.474263Z",
     "shell.execute_reply": "2022-01-19T00:10:29.473558Z",
     "shell.execute_reply.started": "2022-01-19T00:10:22.362751Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n",
    "    num_classes = 2  # 1 class (starfish) + background\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Load the trained weights\n",
    "    model.load_state_dict(torch.load(WEIGHTS_FILE))\n",
    "    model.eval()\n",
    "\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T00:10:29.476921Z",
     "iopub.status.busy": "2022-01-19T00:10:29.476392Z",
     "iopub.status.idle": "2022-01-19T00:10:29.483099Z",
     "shell.execute_reply": "2022-01-19T00:10:29.481291Z",
     "shell.execute_reply.started": "2022-01-19T00:10:29.476884Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"../input/tensorflow-great-barrier-reef/train_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T00:10:29.486376Z",
     "iopub.status.busy": "2022-01-19T00:10:29.486162Z",
     "iopub.status.idle": "2022-01-19T00:10:29.520632Z",
     "shell.execute_reply": "2022-01-19T00:10:29.519814Z",
     "shell.execute_reply.started": "2022-01-19T00:10:29.486351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment this if we want to submit the scores\n",
    "\n",
    "#import greatbarrierreef\n",
    "# import PIL.Image\n",
    "# env = greatbarrierreef.make_env()\n",
    "# iter_test = env.iter_test() \n",
    "\n",
    "# for (pixel_array, df_pred) in iter_test:  # iterate through all test set images\n",
    "#     df_pred['annotations'] = predict(model, pixel_array)\n",
    "#     env.predict(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T00:10:29.522554Z",
     "iopub.status.busy": "2022-01-19T00:10:29.522344Z",
     "iopub.status.idle": "2022-01-19T00:10:29.917417Z",
     "shell.execute_reply": "2022-01-19T00:10:29.916775Z",
     "shell.execute_reply.started": "2022-01-19T00:10:29.522530Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/csvfile/train-test.csv\")\n",
    "\n",
    "# Turn annotations from strings into lists of dictionaries\n",
    "df['annotations'] = df['annotations'].apply(eval)\n",
    "\n",
    "# Create the image path for the row\n",
    "df['image_path'] = \"video_\" + df['video_id'].astype(str) + \"/\" + df['video_frame'].astype(str) + \".jpg\"\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T00:10:29.919920Z",
     "iopub.status.busy": "2022-01-19T00:10:29.918423Z",
     "iopub.status.idle": "2022-01-19T00:10:29.927251Z",
     "shell.execute_reply": "2022-01-19T00:10:29.926550Z",
     "shell.execute_reply.started": "2022-01-19T00:10:29.919880Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train, df_val = df[df['is_train']], df[~df['is_train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T00:10:29.928955Z",
     "iopub.status.busy": "2022-01-19T00:10:29.928649Z",
     "iopub.status.idle": "2022-01-19T00:10:29.953718Z",
     "shell.execute_reply": "2022-01-19T00:10:29.953134Z",
     "shell.execute_reply.started": "2022-01-19T00:10:29.928918Z"
    }
   },
   "outputs": [],
   "source": [
    "# The model doesn't support images with no annotations\n",
    "# It raises an error that suggest that it just doesn't support them:\n",
    "# V    alueError: No ground-truth boxes available for one of the images during training\n",
    "# I'm dropping those images for now\n",
    "# https://discuss.pytorch.org/t/fasterrcnn-images-with-no-objects-present-cause-an-error/117974/3\n",
    "df_train = df_train[df_train.annotations.str.len() > 0 ].reset_index(drop=True)\n",
    "df_val = df_val[df_val.annotations.str.len() > 0 ].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T00:10:29.955119Z",
     "iopub.status.busy": "2022-01-19T00:10:29.954814Z",
     "iopub.status.idle": "2022-01-19T00:10:29.960610Z",
     "shell.execute_reply": "2022-01-19T00:10:29.959939Z",
     "shell.execute_reply.started": "2022-01-19T00:10:29.955083Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.shape[0], df_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T00:10:29.962345Z",
     "iopub.status.busy": "2022-01-19T00:10:29.961915Z",
     "iopub.status.idle": "2022-01-19T00:10:29.980381Z",
     "shell.execute_reply": "2022-01-19T00:10:29.979651Z",
     "shell.execute_reply.started": "2022-01-19T00:10:29.962305Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReefDataset:\n",
    "\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def can_augment(self, boxes):\n",
    "        \"\"\" Check if bounding boxes are OK to augment\n",
    "        \n",
    "        \n",
    "        For example: image_id 1-490 has a bounding box that is partially outside of the image\n",
    "        It breaks albumentation\n",
    "        Here we check the margins are within the image to make sure the augmentation can be applied\n",
    "        \"\"\"\n",
    "        \n",
    "        box_outside_image = ((boxes[:, 0] < 0).any() or (boxes[:, 1] < 0).any() \n",
    "                             or (boxes[:, 2] > 1280).any() or (boxes[:, 3] > 720).any())\n",
    "        return not box_outside_image\n",
    "\n",
    "    def get_boxes(self, row):\n",
    "        \"\"\"Returns the bboxes for a given row as a 3D matrix with format [x_min, y_min, x_max, y_max]\"\"\"\n",
    "        \n",
    "        boxes = pd.DataFrame(row['annotations'], columns=['x', 'y', 'width', 'height']).astype(float).values\n",
    "        \n",
    "        # Change from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        return boxes\n",
    "    \n",
    "    def get_image(self, row):\n",
    "        \"\"\"Gets the image for a given row\"\"\"\n",
    "        \n",
    "        image = cv2.imread(f'{BASE_DIR}/{row[\"image_path\"]}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        row = self.df.iloc[i]\n",
    "        image = self.get_image(row)\n",
    "        boxes = self.get_boxes(row)\n",
    "        \n",
    "        n_boxes = boxes.shape[0]\n",
    "        \n",
    "        # Calculate the area\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        \n",
    "        target = {\n",
    "            'boxes': torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            'area': torch.as_tensor(area, dtype=torch.float32),\n",
    "            \n",
    "            'image_id': torch.tensor([i]),\n",
    "            \n",
    "            # There is only one class\n",
    "            'labels': torch.ones((n_boxes,), dtype=torch.int64),\n",
    "            \n",
    "            # Suppose all instances are not crowd\n",
    "            'iscrowd': torch.zeros((n_boxes,), dtype=torch.int64)            \n",
    "        }\n",
    "\n",
    "        if self.transforms and self.can_augment(boxes):\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': target['labels']\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "            if n_boxes > 0:\n",
    "                target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "        else:\n",
    "            image = ToTensorV2(p=1.0)(image=image)['image']\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T00:10:29.983235Z",
     "iopub.status.busy": "2022-01-19T00:10:29.982988Z",
     "iopub.status.idle": "2022-01-19T00:10:29.992502Z",
     "shell.execute_reply": "2022-01-19T00:10:29.991813Z",
     "shell.execute_reply.started": "2022-01-19T00:10:29.983202Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Flip(0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T00:10:29.994122Z",
     "iopub.status.busy": "2022-01-19T00:10:29.993829Z",
     "iopub.status.idle": "2022-01-19T00:10:30.002701Z",
     "shell.execute_reply": "2022-01-19T00:10:30.002055Z",
     "shell.execute_reply.started": "2022-01-19T00:10:29.994087Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define datasets\n",
    "ds_train = ReefDataset(df_train, get_train_transform())\n",
    "ds_val = ReefDataset(df_val, get_valid_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T00:10:30.004262Z",
     "iopub.status.busy": "2022-01-19T00:10:30.004017Z",
     "iopub.status.idle": "2022-01-19T00:10:30.044284Z",
     "shell.execute_reply": "2022-01-19T00:10:30.043563Z",
     "shell.execute_reply.started": "2022-01-19T00:10:30.004231Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's get an interesting one ;)\n",
    "df_train[df_train.annotations.str.len() > 12].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T00:10:30.045842Z",
     "iopub.status.busy": "2022-01-19T00:10:30.045571Z",
     "iopub.status.idle": "2022-01-19T00:10:30.151388Z",
     "shell.execute_reply": "2022-01-19T00:10:30.150672Z",
     "shell.execute_reply.started": "2022-01-19T00:10:30.045809Z"
    }
   },
   "outputs": [],
   "source": [
    "image, targets = ds_train[2200]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T00:10:30.152965Z",
     "iopub.status.busy": "2022-01-19T00:10:30.152710Z",
     "iopub.status.idle": "2022-01-19T00:10:30.680053Z",
     "shell.execute_reply": "2022-01-19T00:10:30.679424Z",
     "shell.execute_reply.started": "2022-01-19T00:10:30.152931Z"
    }
   },
   "outputs": [],
   "source": [
    "boxes = targets['boxes'].cpu().numpy().astype(np.int32)\n",
    "img = image.permute(1,2,0).cpu().numpy()\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "for box in boxes:\n",
    "    cv2.rectangle(img,\n",
    "                  (box[0], box[1]),\n",
    "                  (box[2], box[3]),\n",
    "                  (220, 0, 0), 3)\n",
    "    \n",
    "ax.set_axis_off()\n",
    "ax.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T00:10:30.681214Z",
     "iopub.status.busy": "2022-01-19T00:10:30.680980Z",
     "iopub.status.idle": "2022-01-19T00:10:30.692517Z",
     "shell.execute_reply": "2022-01-19T00:10:30.691782Z",
     "shell.execute_reply.started": "2022-01-19T00:10:30.681183Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "print(\"ds_val: \", ds_val)\n",
    "dl_train = DataLoader(ds_train, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "dl_val = DataLoader(ds_val, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "print(\"dl_val: \", dl_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T00:10:30.694561Z",
     "iopub.status.busy": "2022-01-19T00:10:30.694071Z",
     "iopub.status.idle": "2022-01-19T00:10:30.724125Z",
     "shell.execute_reply": "2022-01-19T00:10:30.723259Z",
     "shell.execute_reply.started": "2022-01-19T00:10:30.694528Z"
    }
   },
   "outputs": [],
   "source": [
    "detection_threshold = 0\n",
    "import torch\n",
    "from collections import Counter\n",
    "#from iou import intersection_over_union\n",
    "\n",
    "def format_prediction_string(boxes, scores):\n",
    "    # Format as specified in the evaluation page\n",
    "    pred_strings = []\n",
    "    for j in zip(scores, boxes):\n",
    "        pred_strings.append(\"{0:.2f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n",
    "\n",
    "    return \" \".join(pred_strings)\n",
    "\n",
    "\n",
    "def predict(model, pixel_array):\n",
    "    # Predictions for a single image\n",
    "    \n",
    "    # Apply all the transformations that are required\n",
    "    #pixel_array = pixel_array.astype(np.float32) / 255.\n",
    "    tensor_img = ToTensorV2(p=1.0)(image=pixel_array)['image'].unsqueeze(0)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tensor_img.to(device))[0]\n",
    "        \n",
    "    #print(\"outputs loob: \", outputs)\n",
    "    # Move predictions to cpu and numpy\n",
    "    boxes = outputs['boxes'].data.cpu().numpy()\n",
    "    scores = outputs['scores'].data.cpu().numpy()\n",
    "    \n",
    "    # Filter predictions with low score\n",
    "    boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "    scores = scores[scores >= detection_threshold]\n",
    "    #print(\"boxes loob before: \", boxes)\n",
    "    #print(\"scores loob before: \", scores)\n",
    "    \n",
    "    # Go back from x_min, y_min, x_max, y_max to x_min, y_min, w, h\n",
    "    #boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "    #boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "  \n",
    "    # Format results as requested in the Evaluation tab\n",
    "    #return format_prediction_string(boxes, scores)\n",
    "    return boxes, scores\n",
    "\n",
    "def intersection_over_union(gt_box, pred_box):\n",
    "    inter_box_top_left = [max(gt_box[0], pred_box[0]), max(gt_box[1], pred_box[1])]\n",
    "    inter_box_bottom_right = [min(gt_box[0]+gt_box[2], pred_box[0]+pred_box[2]), min(gt_box[1]+gt_box[3], pred_box[1]+pred_box[3])]\n",
    "\n",
    "    inter_box_w = inter_box_bottom_right[0] - inter_box_top_left[0]\n",
    "    inter_box_h = inter_box_bottom_right[1] - inter_box_top_left[1]\n",
    "\n",
    "    intersection = inter_box_w * inter_box_h\n",
    "    union = gt_box[2] * gt_box[3] + pred_box[2] * pred_box[3] - intersection\n",
    "    \n",
    "    iou = intersection / union\n",
    "\n",
    "    return iou, intersection, union\n",
    "\n",
    "def mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"corners\", num_classes=2):\n",
    "    average_precisions = []\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "        \n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "        \n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "                \n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "        \n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "            \n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "        \n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "            \n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "            \n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "            \n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum,(TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([0]), recalls))\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "        \n",
    "    return sum(average_precisions) / len(average_precisions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T00:10:30.726291Z",
     "iopub.status.busy": "2022-01-19T00:10:30.725743Z",
     "iopub.status.idle": "2022-01-19T00:12:17.000608Z",
     "shell.execute_reply": "2022-01-19T00:12:16.999921Z",
     "shell.execute_reply.started": "2022-01-19T00:10:30.726254Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../input/meanaverageprecision/')\n",
    "import mAP\n",
    "from collections import Counter\n",
    "\n",
    "iou_thresholds = 0.5\n",
    "form='pascal_voc'\n",
    "scoreArray = []\n",
    "iou_threshold = 0.5\n",
    "TP = []\n",
    "FP = []\n",
    "TN = []\n",
    "FN = []\n",
    "for i in range(len(ds_val)):\n",
    "    \n",
    "#for i in range(0, len(ds_val), 8):\n",
    "    image, targets = ds_val[i]\n",
    "    boxes = targets['boxes'].cpu().numpy().astype(np.int32)\n",
    "    sample = image.permute(1,2,0).cpu().numpy()\n",
    "    print(\"i: \", i)\n",
    "    #print(\"len boxes: \", len(boxes))\n",
    "    prediction_boxes, prediction_scores = predict(model, sample)\n",
    "    #mean_average_precision(prediction_boxes, boxes)\n",
    "    \n",
    "    #image_precision = mAP.calculate_image_precision(boxes, prediction_boxes,thresholds=iou_thresholds,form=form)\n",
    "    \n",
    "    best_iou = 0\n",
    "    amount_bboxes = 0\n",
    "    print(\"actual: \", boxes)\n",
    "    print(\"prediction: \", prediction_boxes)\n",
    "    print(\"prediction scores: \", prediction_scores)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    for idx, ground_box in enumerate(boxes):\n",
    "        #print(\"idx: \", idx)\n",
    "        #print(\"ground_box: \", ground_box)\n",
    "        if len(prediction_boxes) == 0:\n",
    "            #print(\"wala\")\n",
    "            FN.append(1)\n",
    "            #print(\"\\n\")\n",
    "        else:\n",
    "            for pred_box in prediction_boxes:\n",
    "                #if prediction_boxes !=\n",
    "                #print(\"i_c: \", i_c)\n",
    "                #print(\"prediction_box ind: \", pred_box)\n",
    "                IoUcomp, intersection, union = intersection_over_union(ground_box, pred_box)\n",
    "                #print(\"IoU: \", IoUcomp)\n",
    "\n",
    "\n",
    "                if IoUcomp > best_iou:\n",
    "                    best_iou = IoUcomp\n",
    "                    #print(\"best_iou 1: \", best_iou)\n",
    "\n",
    "                    if best_iou > iou_threshold:\n",
    "                        if amount_bboxes == 0:\n",
    "                            TP.append(1)\n",
    "                            amount_bboxes = 1\n",
    "\n",
    "                        else:\n",
    "                            FP.append(1)\n",
    "\n",
    "                    else:\n",
    "                        FP.append(1)\n",
    "\n",
    "TP_sum = sum(TP)\n",
    "FP_sum = sum(FP)\n",
    "FN_sum = sum(FN)\n",
    "print(\"TP: \", TP_sum)\n",
    "print(\"FP: \", FP_sum)\n",
    "print(\"FN: \", FN_sum)\n",
    "recalls = TP_sum / (TP_sum + FN_sum)\n",
    "precisions = TP_sum / (TP_sum + FP_sum)\n",
    "\n",
    "print(\"precision: \", precisions)\n",
    "print(\"recall: \", recalls)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
